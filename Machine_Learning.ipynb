{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Machine Learning**\n",
        "---"
      ],
      "metadata": {
        "id": "FTopbooWTehK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "“Learning” in Machine Learning = the process of a machine finding mathematical patterns from data, by reducing the error between predictions and reality."
      ],
      "metadata": {
        "id": "Drkuh-lMTkhu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Type of Machine Learning\n",
        "---"
      ],
      "metadata": {
        "id": "d1bJEMCRVUsa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Supervised Learning**\n",
        "\n",
        "Models that learn from data that already has labels (correct answer)\n",
        "\n",
        "Example:\n",
        "\n",
        "| Fitur (Input)     | Label (Output)  |\n",
        "| ----------------- | --------------- |\n",
        "| Luas rumah: 100m² | Harga: 800 juta |\n",
        "| Luas rumah: 150m² | Harga: 1 M      |\n",
        "| Luas rumah: 200m² | Harga: 1.3 M    |\n",
        "\n",
        "model will learn the relation between fitur and label, and they can predict a new label.\n",
        "\n",
        "\n",
        "**Type of Supervised Learning**\n",
        "* Regression\n",
        "\n",
        "The output is usually a continuous value or a number. Examples include house price predictions and blood sugar level predictions.\n",
        "\n",
        "Some commonly used algorithms are linear regression, decision trees, random forests, and SVR.\n",
        "* Classification\n",
        "\n",
        "The output is in the form of categories. For example, whether an email is spam or not, or whether an image is a dog or a cat.\n",
        "\n",
        "Algorithms commonly used are logistic regression, KKN, and naive Bayes."
      ],
      "metadata": {
        "id": "-ObHFSYhVfev"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Unsupervised Learning**\n",
        "\n",
        "Data is unlabeled, generally, the model must determine its own patterns.\n",
        "\n",
        "**Types of unsupervised learning**\n",
        "\n",
        "* Clustering\n",
        "\n",
        "Grouping similar data, such as segmentation, grouping music types based on sound patterns.\n",
        "\n",
        "Popular algorithms commonly used are K-means, hierarchical clustering, and dbscan.\n",
        "\n",
        "* Dimension reduction\n",
        "\n",
        "Reducing the number of features while retaining important information.\n",
        "\n",
        "Some algorithms used include PCA, t-sne, and autocoder."
      ],
      "metadata": {
        "id": "29RhlFLFXcXD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reincofce Learning**\n",
        "\n",
        "The model will learn through actions in the environment, receiving rewards or punishments.\n",
        "\n",
        "Main components\n",
        "\n",
        "* Agent: the one who learns or makes decisions\n",
        "* Environment: the world where the agent acts\n",
        "* Actions (A): what the agent does\n",
        "* State (S): the current environmental condition\n",
        "* Reward (R): the value obtained after the action\n",
        "\n",
        "Algorithms commonly used are q-learning, deep q-network, etc."
      ],
      "metadata": {
        "id": "-0DYnH2vYdoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bias Variance Trade-off\n",
        "---"
      ],
      "metadata": {
        "id": "Mx4VAa8XaxG_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Risiko       | Means                                                             | Tendency       |\n",
        "| ------------ | ---------------------------------------------------------------- | ------------ |\n",
        "| **Bias**     | The model is too *simple* to capture the data pattern.              | Underfitting |\n",
        "| **Variance** | The model is too *complex* and too sensitive to the training data. | Overfitting  |\n",
        "\n",
        "The trade-off serves to balance the two so that the model is complex enough to learn important patterns but not too noisy.\n",
        "\n",
        "*`Total Error = Bias^2 + Variance + Irreducible Error`*\n",
        "\n",
        "* Bias^2 = how far the model's prediction is from the actual value.\n",
        "* Variance = how much the model's results change if the training data is slightly changed.\n",
        "* Irreducible Error = inherent noise in the data.\n",
        "\n",
        "Example:\n",
        "| Tree Depth | Bias     | Variance | Condition  |\n",
        "| ---------- | -------- | -------- | -------- |\n",
        "| 2          | High   | Low   | Underfit |\n",
        "| 30         | Low   | High   | Overfit  |\n",
        "| 8–12       | = | = | Optimal  |\n",
        "\n",
        "Control Bias dan Variance\n",
        "\n",
        "| Approach                                           | Reduce Bias | Reduce Variance       |\n",
        "| ---------------------------------------------------- | --------------- | ------------------------- |\n",
        "| Add features | Yes | No |\n",
        "| Reduce features (feature selection) | No | Yes |\n",
        "| Increase model complexity (larger neural network) | Yes | No |\n",
        "| Regularization (L1/L2, dropout) | No | Yes |\n",
        "| Add training data | Yes (sometimes) | Yes (stabilize the model) |\n",
        "| Bagging (Random Forest) | No | Yes |\n",
        "| Boosting (XGBoost) | Yes | No |\n",
        "| Cross-validation | No | Yes (measure generalization) |"
      ],
      "metadata": {
        "id": "fV-8mB8lVxpn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Overfitting and Underfitting**\n",
        "\n",
        "| Condition       | Training Error | Testing Error     |\n",
        "| ------------- | -------------- | ----------------- |\n",
        "| Underfitting  | High         | High            |\n",
        "| Overfitting   | Low         | High            |\n",
        "| Correct Fit | Low         | Low (equal) |\n",
        "\n",
        "\n",
        "Causes\n",
        "\n",
        "| Underfitting | Overfitting |\n",
        "| -------------------------------------------------------------- | --------------------------------------------------------- |\n",
        "| Model too simple (Linear Regression for nonlinear data) | Model too complex (too many parameters/layers) |\n",
        "| Features not informative | Too many features (no regularization) |\n",
        "| Too little data | Too little data, but large model |\n",
        "| Training too short | Training too long (memorizing data) |\n",
        "| Regularization too strong | Regularization too weak |\n",
        "\n",
        "How to fix underfitting\n",
        "| Strategy | Explanation |\n",
        "| ---------------------------- | --------------------------------------------------- |\n",
        "| Increase model complexity | For example, from linear to random forest/neural network |\n",
        "| Add relevant features | Improve feature engineering |\n",
        "| Reduce regularization | For example, lower the λ value in Lasso/Ridge |\n",
        "| Train longer | Try more epochs (neural network) |\n",
        "| Try non-linear transformations | Polynomial, kernel, log transform |\n",
        "\n",
        "How to fix overfitting\n",
        "| Strategy | Explanation |\n",
        "| ------------------------------------ | ------------------------------------------------------------------- |\n",
        "| Add data | More examples → model learns general patterns |\n",
        "| Regularization | L1 (Lasso), L2 (Ridge), Dropout, Early Stopping |\n",
        "| Reduce model complexity | Reduce depth in decision trees, layers in NN |\n",
        "| Feature selection | Remove irrelevant features |\n",
        "| Cross-validation | Detect overfitting early |\n",
        "| Data augmentation | Add variety to data (images, text, etc.) |\n",
        "| Ensemble | Use Random Forest, Bagging, Averaging to reduce variance |"
      ],
      "metadata": {
        "id": "2WeYGo8rYrPn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simple Machine Learning Pipeline\n",
        "---"
      ],
      "metadata": {
        "id": "Wr1iliHWbRDd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Problem Definition\n",
        "\n",
        "* Objective: Define the task (classification/regression/clustering/etc.), success metrics (precision, recall, F1-Score, etc), constraints (latency, privacy, budget), and scope.\n",
        "\n",
        "* Clear output: for example, “predict house price (regression), MAPE < 10%, inference < 100ms.”\n",
        "\n",
        "* Stakeholders, label sources, business impact.\n",
        "\n",
        "2. Data Collection & Ingestion\n",
        "\n",
        "* Objective: Collect relevant data from all sources.\n",
        "\n",
        "* Sources: databases, CSV, APIs, sensors, scraping, open datasets.\n",
        "\n",
        "* Record metadata: schema, timestamp, owner, refresh rate.\n",
        "\n",
        "* Storage raw data (immutable) + logs. Use a structured format (Parquet/CSV/SQL).\n",
        "\n",
        "3. Data Understanding / Exploratory Data Analysis (EDA)\n",
        "\n",
        "* Objective: Understand distribution, missingness, noise, correlation, and quality issues.\n",
        "\n",
        "* Summary statistics (mean, median, standard deviation), histogram, boxplot.\n",
        "\n",
        "* Correlation (Pearson/Spearman), heatmap.\n",
        "\n",
        "* Per-class visualization for classification.\n",
        "\n",
        "* Find bias, class imbalance, potential drift.\n",
        "\n",
        "4. Data cleaning & preprocessing\n",
        "\n",
        "* Goal: Improve quality so the model learns from signal, not noise.\n",
        "\n",
        "* Handle missing data: drop/impute (mean/median/KNN/iterative).\n",
        "\n",
        "* Outliers: Detect (IQR, z-score) → decide remove/clip/transform.\n",
        "\n",
        "* Type consistency, normalize/standardize, categorical encoding (one-hot, ordinal, target encoding).\n",
        "\n",
        "* Timestamp handling: Extract features (day, hour, lag features).\n",
        "\n",
        "5. Feature engineering\n",
        "\n",
        "* Goal: Create data representations that facilitate model learning.\n",
        "\n",
        "* Domain features: ratio, aggregation, lag features (time series), interaction terms.\n",
        "\n",
        "* Encoding: embedding for high-cardinality categorical.\n",
        "\n",
        "* Dimensionality reduction: PCA / UMAP / feature selection.\n",
        "\n",
        "* Create pipelines so feature transformations are reproducible.\n",
        "\n",
        "6. Train/validation/test splitting\n",
        "\n",
        "* Goal: estimation of model generalization.\n",
        "\n",
        "* Random split (i.i.d. data) — train/val/test (80/10/10 or as appropriate).\n",
        "\n",
        "* Time series: use time-based split (walk-forward, expanding window).\n",
        "\n",
        "* Cross-validation: K-fold, stratified K-fold (classification), grouped CV (data leakage prevention).\n",
        "\n",
        "Important: test set strictly held-out (only final eval).\n",
        "\n",
        "7. Model selection\n",
        "\n",
        "* Goal: select candidate models & baselines.\n",
        "\n",
        "* Baselines: mean predictor, logistic regression, decision tree — always start from the baseline.\n",
        "\n",
        "* Candidates: linear models, tree ensembles (RandomForest, XGBoost, LightGBM), SVM, neural networks.\n",
        "\n",
        "* For image/text: CNN/transformer/embeddings.\n",
        "\n",
        "* Consider complexity vs latency vs interpretability.\n",
        "\n",
        "8. Training & hyperparameter tuning\n",
        "\n",
        "* Goal: parameter optimization for best performance.\n",
        "\n",
        "* Loss function according to task (MSE, cross-entropy, ranking loss).\n",
        "\n",
        "* Optimizers (SGD/Adam) for NN; early stopping.\n",
        "\n",
        "* Hyperparameter tuning: grid search, random search, Bayesian (Optuna), Hyperopt.\n",
        "\n",
        "* Use cross-validation and keep validation sets separate.\n",
        "\n",
        "9. Evaluation & metrics\n",
        "\n",
        "* Objective: assess performance and conformity to requirements.\n",
        "\n",
        "* Regression: MSE, RMSE, MAE, MAPE, R².\n",
        "\n",
        "* Classification: accuracy, precision, recall, F1, ROC-AUC, PR-AUC (imbalanced).\n",
        "\n",
        "* Calibration: reliability diagrams, Brier score.\n",
        "\n",
        "* Confusion matrix, per-class metrics, error analysis (case study).\n",
        "\n",
        "* Fairness & bias checks (demographic parity, equalized odds) where relevant.\n",
        "\n",
        "10. Model interpretation & explainability\n",
        "\n",
        "* Goal: understand what the model learns.\n",
        "\n",
        "* Feature importance (Tree SHAP, permutation importance).\n",
        "\n",
        "* SHAP / LIME for local & global explanations.\n",
        "\n",
        "* Partial dependence plots, ICE plots.\n",
        "\n",
        "* Document feature drift risk & spurious correlations."
      ],
      "metadata": {
        "id": "rs3Wfk7VbV9-"
      }
    }
  ]
}